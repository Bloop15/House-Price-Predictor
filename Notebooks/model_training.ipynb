{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4cbc60",
   "metadata": {},
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4ff48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fdbc628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbd139",
   "metadata": {},
   "source": [
    "### Load the Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "695914c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f7149c",
   "metadata": {},
   "source": [
    "### Split the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42d5946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (1163, 199)\n",
      "Shape of y_train:  (1163,)\n",
      "Shape of X_test:  (291, 199)\n",
      "Shape of y_test:  (291,)\n"
     ]
    }
   ],
   "source": [
    "# Define the features(X) and the target variable(y)\n",
    "X = df.drop(['Id', 'SalePrice', 'SalePrice_Log'], axis=1)\n",
    "y = df['SalePrice_Log']\n",
    "\n",
    "# Split the data into a training set(80%) and a testing set(20%)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62372190",
   "metadata": {},
   "source": [
    "### Train the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d00c8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training linear models.....\n",
      "-Linear Regression trained\n",
      "-Ridge trained\n",
      "-Lasso trained\n",
      "-ElasticNet trained\n",
      "All models trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Train the Baseline (Linear) model and Regularized (Ridge, Lasso, ElasticNet) models\n",
    "models= {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(random_state=42),\n",
    "    \"Lasso\": Lasso(random_state=42),\n",
    "    \"ElasticNet\": ElasticNet(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"Training linear models.....\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"-{name} trained\")\n",
    "print(\"All models trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4f1e404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Tree-based and Boosting models.....\n",
      "-Decision Tree trained\n",
      "-Random Forest trained\n",
      "-XGBoost trained\n",
      "All models trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Train the Tree-based and Boosting models\n",
    "models[\"Decision Tree\"]= DecisionTreeRegressor(random_state=42)\n",
    "models[\"Random Forest\"]= RandomForestRegressor(random_state=42)\n",
    "models[\"XGBoost\"]= XGBRegressor(random_state=42)\n",
    "\n",
    "print(\"Training Tree-based and Boosting models.....\")\n",
    "for name, model in models.items():\n",
    "    if name not in [\"Linear Regression\", \"Ridge\", \"Lasso\", \"ElasticNet\"]:\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"-{name} trained\")\n",
    "print(\"All models trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f15ee",
   "metadata": {},
   "source": [
    "### Evaluate the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a32475f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models with 5-fold cross validation\n",
      "--------------------------------------------------\n",
      "Linear Regression Evaluation: \n",
      "    Mean RMSE: 0.1235\n",
      "    Mean R2: 0.9005\n",
      "--------------------------------------------------\n",
      "Ridge Evaluation: \n",
      "    Mean RMSE: 0.1164\n",
      "    Mean R2: 0.9117\n",
      "--------------------------------------------------\n",
      "Lasso Evaluation: \n",
      "    Mean RMSE: 0.3942\n",
      "    Mean R2: -0.0005\n",
      "--------------------------------------------------\n",
      "ElasticNet Evaluation: \n",
      "    Mean RMSE: 0.3942\n",
      "    Mean R2: -0.0005\n",
      "--------------------------------------------------\n",
      "Decision Tree Evaluation: \n",
      "    Mean RMSE: 0.2133\n",
      "    Mean R2: 0.7055\n",
      "--------------------------------------------------\n",
      "Random Forest Evaluation: \n",
      "    Mean RMSE: 0.1408\n",
      "    Mean R2: 0.8718\n",
      "--------------------------------------------------\n",
      "XGBoost Evaluation: \n",
      "    Mean RMSE: 0.1392\n",
      "    Mean R2: 0.8744\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "kf= KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Dictionary to store the results\n",
    "results={}\n",
    "\n",
    "# Evaluate each model using Cross-Validation\n",
    "print(\"Evaluating models with 5-fold cross validation\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for name, model in models.items():\n",
    "    rmse_scores= np.sqrt(-cross_val_score(model, X_train, y_train, cv= kf, scoring= 'neg_mean_squared_error'))\n",
    "    r2_scores= cross_val_score(model, X_train, y_train, cv= kf, scoring= 'r2')\n",
    "\n",
    "    results[name]= {\n",
    "        \"Mean RMSE\": rmse_scores.mean(),\n",
    "        \"Std RMSE\": rmse_scores.std(),\n",
    "        \"Mean R2\": r2_scores.mean(),\n",
    "        \"Std R2\": r2_scores.std()\n",
    "    }\n",
    "\n",
    "    print(f\"{name} Evaluation: \")\n",
    "    print(f\"    Mean RMSE: {results[name]['Mean RMSE']:.4f}\")\n",
    "    print(f\"    Mean R2: {results[name]['Mean R2']:.4f}\")\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441ba01",
   "metadata": {},
   "source": [
    "## Model Evaluation Results (5-Fold Cross Validation)\n",
    "\n",
    "The following models were trained and evaluated using RMSE (Root Mean Squared Error) and R² (coefficient of determination):\n",
    "\n",
    "### Observations\n",
    "\n",
    "- **Linear Regression**: Strong baseline with high R² (0.9005), but limited in capturing complex nonlinear patterns.  \n",
    "- **Ridge Regression**: Best-performing model overall with the lowest RMSE (0.1164) and highest R² (0.9117), showing the benefit of regularization.  \n",
    "- **Lasso Regression**: Extremely poor performance (negative R²), indicating severe underfitting due to excessive regularization.  \n",
    "- **ElasticNet**: Similar to Lasso, failed to learn meaningful relationships, likely due to strong penalty terms.  \n",
    "- **Decision Tree**: Moderate performance (R² ≈ 0.71), prone to overfitting and not competitive with Ridge or ensembles.  \n",
    "- **Random Forest**: Solid performance (RMSE = 0.1408, R² = 0.8718), effectively models nonlinearity but less accurate than Ridge or XGBoost.  \n",
    "- **XGBoost**: Strong ensemble model (RMSE = 0.1392, R² = 0.8744), very close to Random Forest but still slightly behind Ridge.  \n",
    "\n",
    "---\n",
    "\n",
    "### Best Performing Models\n",
    "- **Ridge Regression** – best overall, offering the highest predictive accuracy.  \n",
    "- **XGBoost** – best ensemble method, competitive with Ridge.  \n",
    "- **Random Forest** – reliable model, but slightly weaker than XGBoost.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc636c",
   "metadata": {},
   "source": [
    "### Broad HyperParameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fdf94d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing broad search for Ridge....\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters for Ridge: {'alpha': 10}\n",
      "Best cross-validated RMSE for Ridge:  0.1142\n",
      "\n",
      "Performing broad search for RandomForest....\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters for RandomForest: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_depth': None}\n",
      "Best cross-validated RMSE for RandomForest:  0.1397\n",
      "\n",
      "Performing broad search for XGBoost....\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best parameters for XGBoost: {'subsample': 0.6, 'reg_lambda': 0.5, 'reg_alpha': 0, 'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "Best cross-validated RMSE for XGBoost:  0.1185\n"
     ]
    }
   ],
   "source": [
    "ridge_param_grid = {'alpha': [0.1, 0.5, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'learning_rate':[0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1, 10],\n",
    "    'reg_lambda': [0.5, 1, 5, 10]\n",
    "}\n",
    "\n",
    "model_params= {\n",
    "    'Ridge': (Ridge(random_state=42), ridge_param_grid),\n",
    "    'RandomForest': (RandomForestRegressor(random_state=42), rf_param_grid),\n",
    "    'XGBoost': (XGBRegressor(random_state=42), xgb_param_grid)\n",
    "}\n",
    "\n",
    "best_params_random= {}\n",
    "best_scores_random= {}\n",
    "\n",
    "for name, (model, param_grid) in model_params.items():\n",
    "    print(f\"\\nPerforming broad search for {name}....\")\n",
    "    random_search= RandomizedSearchCV(\n",
    "        estimator= model,\n",
    "        param_distributions= param_grid,\n",
    "        n_iter= 50,\n",
    "        cv= 5,\n",
    "        scoring= 'neg_mean_squared_error',\n",
    "        n_jobs= -1,\n",
    "        verbose= 1,\n",
    "        random_state= 42\n",
    "    )\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    best_params_random[name]= random_search.best_params_\n",
    "    best_scores_random[name]= np.sqrt(-random_search.best_score_)\n",
    "\n",
    "    print(f\"Best parameters for {name}: {best_params_random[name]}\")\n",
    "    print(f\"Best cross-validated RMSE for {name}: {best_scores_random[name]: .4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f84d3",
   "metadata": {},
   "source": [
    "### Fine-Tuning HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45c769bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing fine-tuning for Ridge....\n",
      "Best parameters for Ridge (fine-tuned): {'alpha': 13}\n",
      "Best cross-validated RMSE for Ridge (fine-tuned):  0.1142\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Ridge Regression\n",
    "ridge_param_grid_fine = {'alpha': [8, 9, 10, 11, 12, 13]}\n",
    "ridge_gs_fine= GridSearchCV(Ridge(random_state= 42), ridge_param_grid_fine, cv= 5, scoring= 'neg_mean_squared_error', n_jobs= -1)\n",
    "\n",
    "print(\"Performing fine-tuning for Ridge....\")\n",
    "ridge_gs_fine.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for Ridge (fine-tuned): {ridge_gs_fine.best_params_}\")\n",
    "print(f\"Best cross-validated RMSE for Ridge (fine-tuned): {np.sqrt(-ridge_gs_fine.best_score_): .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66c63d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing fine-tuning for XGBoost....\n",
      "Best parameters for XGBoost (fine-tuned): {'colsample_bytree': 0.75, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 550, 'reg_alpha': 0.001, 'reg_lambda': 0.4, 'subsample': 0.6}\n",
      "Best cross-validated RMSE for XGBoost (fine-tuned):  0.1163\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning XGBoost\n",
    "xgb_param_grid_fine = {\n",
    "    'learning_rate': [0.04, 0.05, 0.06],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'n_estimators': [450, 500, 550],\n",
    "    'subsample': [0.5, 0.6, 0.7],\n",
    "    'colsample_bytree': [0.75, 0.8, 0.85],\n",
    "    'reg_lambda': [0.4, 0.5, 0.6],\n",
    "    'reg_alpha': [0, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "xgb_gs_fine = GridSearchCV(XGBRegressor(random_state=42), xgb_param_grid_fine, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "print(\"Performing fine-tuning for XGBoost....\")\n",
    "xgb_gs_fine.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for XGBoost (fine-tuned): {xgb_gs_fine.best_params_}\")\n",
    "print(f\"Best cross-validated RMSE for XGBoost (fine-tuned): {np.sqrt(-xgb_gs_fine.best_score_): .4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe423ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing fine-tuning for Random Forest....\n",
      "Best parameters for Random Forest (fine-tuned): {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 500}\n",
      "Best cross-validated RMSE for Random Forest (fine_tuned):  0.1396\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Random Forest\n",
    "rf_param_grid_fine = {\n",
    "    'n_estimators': [450, 500, 550],\n",
    "    'max_depth': [None, 20, 25],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3]\n",
    "}\n",
    "\n",
    "rf_gs_fine = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid_fine, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "print(\"Performing fine-tuning for Random Forest....\")\n",
    "rf_gs_fine.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters for Random Forest (fine-tuned): {rf_gs_fine.best_params_}\")\n",
    "print(f\"Best cross-validated RMSE for Random Forest (fine_tuned): {np.sqrt(-rf_gs_fine.best_score_): .4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47421402",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "- **Ridge Regression** emerged as the best-performing model, achieving the lowest RMSE. Its strong regularization helped it capture the underlying linear structure of the dataset effectively.  \n",
    "- **XGBoost** delivered competitive results, performing slightly worse than Ridge but still robust. It shows the strength of boosting on structured data.  \n",
    "- **Random Forest** underperformed compared to Ridge and XGBoost, suggesting it struggled to generalize well on this dataset.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb58293",
   "metadata": {},
   "source": [
    "### Retrain Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94a25f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining final models on the complete training dataset with best parameters....\n",
      "-Ridge retrained with best parameters\n",
      "-XGBoost retrained with best parameters\n",
      "-RandomForest retrained with best parameters\n"
     ]
    }
   ],
   "source": [
    "ridge_final= Ridge(random_state= 42, **ridge_gs_fine.best_params_)\n",
    "xgb_final= XGBRegressor(random_state= 42, **xgb_gs_fine.best_params_)\n",
    "rf_final= RandomForestRegressor(random_state= 42, **rf_gs_fine.best_params_)\n",
    "\n",
    "final_models= {\n",
    "    'Ridge': ridge_final,\n",
    "    'XGBoost': xgb_final,\n",
    "    'RandomForest': rf_final\n",
    "}\n",
    "\n",
    "print(\"Retraining final models on the complete training dataset with best parameters....\")\n",
    "for name, model in final_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"-{name} retrained with best parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36eb57",
   "metadata": {},
   "source": [
    "### Final Model Evaluation on Hold-Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ac3bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final models on the hold-out test set....\n",
      "--------------------------------------------------\n",
      "Ridge Final Evaluation:\n",
      "    RMSE: 0.1205\n",
      "    MAE: 0.0801\n",
      "    R2: 0.9154\n",
      "--------------------------------------------------\n",
      "XGBoost Final Evaluation:\n",
      "    RMSE: 0.1285\n",
      "    MAE: 0.0834\n",
      "    R2: 0.9038\n",
      "--------------------------------------------------\n",
      "RandomForest Final Evaluation:\n",
      "    RMSE: 0.1391\n",
      "    MAE: 0.0915\n",
      "    R2: 0.8872\n",
      "--------------------------------------------------\n",
      "Summary of Final Test Set Results: \n",
      "                RMSE     MAE      R2\n",
      "Ridge         0.1205  0.0801  0.9154\n",
      "XGBoost       0.1285  0.0834  0.9038\n",
      "RandomForest  0.1391  0.0915  0.8872\n"
     ]
    }
   ],
   "source": [
    "test_results= {}\n",
    "\n",
    "print(\"Evaluating final models on the hold-out test set....\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for name, model in final_models.items():\n",
    "    y_pred= model.predict(X_test)\n",
    "\n",
    "    rmse= np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae= mean_absolute_error(y_test, y_pred)\n",
    "    r2= r2_score(y_test, y_pred)\n",
    "\n",
    "    test_results[name]= {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2\n",
    "    }\n",
    "\n",
    "    print(f\"{name} Final Evaluation:\")\n",
    "    print(f\"    RMSE: {rmse:.4f}\")\n",
    "    print(f\"    MAE: {mae:.4f}\")\n",
    "    print(f\"    R2: {r2:.4f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "test_results_df= pd.DataFrame(test_results).T\n",
    "print(\"Summary of Final Test Set Results: \")\n",
    "print(test_results_df.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66728a",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "- **Ridge Regression** achieved the lowest RMSE and the highest R², confirming it as the most reliable model for this dataset.  \n",
    "- **XGBoost** delivered competitive performance with RMSE of 0.1285 and R² of 0.9038, showing strong generalization but slightly behind Ridge.  \n",
    "- **Random Forest** had the weakest results, suggesting it struggled to capture the structure of the data compared to Ridge and XGBoost.  \n",
    "\n",
    "**Conclusion:** Ridge Regression is the top-performing model on the hold-out test set, with XGBoost close behind. Random Forest, despite tuning, underperformed in comparison.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0a9d3",
   "metadata": {},
   "source": [
    "### Save the Requried Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caae0c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge model saved to '../models/Ridge_model.pkl'\n",
      "XGBoost model saved to '../models/XGBoost_model.pkl'\n",
      "RandomForest model saved to '../models/RandomForest_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save the three final trained models\n",
    "for name, model in final_models.items():\n",
    "    joblib.dump(model, f'../models/{name}_model.pkl')\n",
    "    print(f\"{name} model saved to '../models/{name}_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46f7d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed test set features and target variable\n",
    "X_test.to_csv('../data/X_test.csv', index= False)\n",
    "y_test.to_csv('../data/y_test.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16ed830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Test Results\n",
    "test_results_df.to_csv('../results/test_results.csv', index= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "House_Price_Predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
